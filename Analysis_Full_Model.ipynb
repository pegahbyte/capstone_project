{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "SVM was used-> not as good results\n",
    "- FastText\n",
    "- Tfidf\n",
    "- Personality\n",
    "- Grammer\n",
    "- VADER Sentiment\n",
    "\n",
    "and their effect on accuracy in prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\MAHAM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#importing libraries for NLP\n",
    "import pandas as pd\n",
    "import os as os\n",
    "import json as js\n",
    "import gzip\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Text Classification Material\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from gensim.models.fasttext import FastText as FT_gensim\n",
    "from gensim.test.utils import datapath\n",
    "import gensim\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#Image processing material\n",
    "import imageio\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from scipy.special import expit as sigmoid\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "#Text Classification Material\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\MAHAM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\MAHAM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os as os\n",
    "import json as js\n",
    "import gzip\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "#importing libraries for NLP\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from collections import Counter\n",
    "\n",
    "#Text Classification Material\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#Text Classification Material\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.fasttext import FastText as FT_gensim\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "from gensim.models import FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe\n",
    "Use the pickled full-feature dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('df_07_08', 'rb') as fr:\n",
    "    df = pickle.load(fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((18810, 68), (18810, 68))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape\n",
    "df2 = df.copy(deep=False)\n",
    "df.shape,df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentence(df):\n",
    "    df[\"split\"]=df[\"corpus\"].apply(lambda x: x.split())\n",
    "split_sentence(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(df.split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes about 10 min:\n",
    "size=500\n",
    "#Movie and TV Data\n",
    "model_m = FastText(size=size, seed=1)  # instantiate\n",
    "model_m.build_vocab(sentences=sentences)\n",
    "model_m.train(sentences=sentences, total_examples=len(sentences), epochs=30)  # train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_vectorizer(sent, model):\n",
    "    '''Uses a pretrained FastText model to create an avage over \n",
    "    a review.'''\n",
    "    sent_vec =[]\n",
    "    numw = 0\n",
    "    for w in sent:\n",
    "        try:\n",
    "            if numw == 0:\n",
    "                sent_vec = model.wv[w]\n",
    "            else:\n",
    "                sent_vec = np.add(sent_vec, model.wv[w])\n",
    "            numw+=1\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return np.asarray(sent_vec) / numw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_sent(df,model,size):\n",
    "    '''Takes a dataframe and adds features: Every dimension of\n",
    "    a model becomes a column. Return new dataframe'''\n",
    "    df[\"avg_sentence\"]=df[\"split\"].apply(lambda x: sent_vectorizer(x, model))\n",
    "    length=df[\"avg_sentence\"].apply(lambda x: len(x))\n",
    "    df2=df[length==size]\n",
    "    for i in range(size):\n",
    "        if size==500:\n",
    "            df2[str(i)] = df2[\"avg_sentence\"].apply(lambda x : x[i])\n",
    "        else:\n",
    "            df2[str(1000+i)] = df2[\"avg_sentence\"].apply(lambda x : x[i])\n",
    "    return(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MAHAM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "# Adding both fasttext-500 and fasttext-1000 features to df's\n",
    "x_m=avg_sent(df, model_m, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle full feature df's\n",
    "with open ('df_07_08_fasttext', 'wb') as fw:\n",
    "    pickle.dump(x_m, fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('df_07_08_fasttext', 'rb') as fr:\n",
    "    x_m = pickle.load(fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['reviewerID', 'asin', 'reviewText', 'overall', 'good', 'Comedy',\n",
       "       'Art House & International', 'Drama', 'Action & Adventure', 'Horror'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_m.columns[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18808, 6)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=x_m[\"length\"]\n",
    "big5 = [\"Extro\", \"Agree\", \"Neuro\", \"Open\", \"Consc\"]\n",
    "X=np.c_[X,x_m[big5]]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18808, 501)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X = x_m[\"length\"]\n",
    "cv = CountVectorizer(max_features = 500, ngram_range=(1,2))\n",
    "XX = cv.fit_transform(x_m[\"corpus\"]).toarray()\n",
    "# using relative word counts instead of absolute ones\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidf = tfidf_transformer.fit_transform(XX).toarray()\n",
    "# tfidf added\n",
    "X = np.c_[tfidf,X] \n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18810, 506)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big5 = [\"Extro\", \"Agree\", \"Neuro\", \"Open\", \"Consc\"]\n",
    "X=np.c_[X,df[big5]]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "X=np.array(df[\"sentiment\"]).reshape(-1, 1)\n",
    "y=df[\"good\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 3)\n",
    "log = LogisticRegression(solver='liblinear')#regularization is applied by default\n",
    "log.fit(X_train, y_train)\n",
    "accuracy=log.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7044125465178097"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18808, 500)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grammar=False\n",
    "personality=False\n",
    "interaction=False\n",
    "size=500\n",
    "if size==500:\n",
    "    dimensions=list(range(size))\n",
    "    embedding = [str(i) for i in dimensions]\n",
    "else:\n",
    "    dimensions=list(range(1000,1000+size))\n",
    "    embedding = [str(i) for i in dimensions]\n",
    "\n",
    "if personality==True:\n",
    "    big5 = [\"Extro\", \"Agree\", \"Neuro\", \"Open\", \"Consc\"]\n",
    "    embedding = big5 + embedding\n",
    "\n",
    "if grammar==True:\n",
    "    grammatical = [\"length\",\"!\",\"?\",\",\",\".\",\" \",'noun', 'adj', 'verb (simple)',\n",
    "'verb (simple past)', 'verb (past participle)', 'verb (gerund)',\n",
    "'adverb', 'preposition']\n",
    "    embedding = grammatical + embedding\n",
    "\n",
    "if interaction==True:\n",
    "    interactions=['Extro_Comedy','Extro_Art House & International', 'Extro_Drama','Extro_Action & Adventure', 'Extro_Horror', \\\n",
    "'Extro_Science Fiction','Extro_Animation', 'Agree_Comedy', 'Agree_Art House & International','Agree_Drama', \\\n",
    "'Agree_Action & Adventure', 'Agree_Horror','Agree_Science Fiction', 'Agree_Animation', 'Neuro_Comedy',\\\n",
    "'Neuro_Art House & International', 'Neuro_Drama','Neuro_Action & Adventure', 'Neuro_Horror', 'Neuro_Science Fiction',\\\n",
    "'Neuro_Animation', 'Open_Comedy', 'Open_Art House & International','Open_Drama', 'Open_Action & Adventure', 'Open_Horror',\\\n",
    "'Open_Science Fiction', 'Open_Animation', 'Consc_Comedy','Consc_Art House & International', 'Consc_Drama',\\\n",
    "'Consc_Action & Adventure', 'Consc_Horror', 'Consc_Science Fiction','Consc_Animation']\n",
    "    embedding = interactions + embedding\n",
    "    \n",
    "X=x_m[embedding]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8290802764486975"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=x_m[\"good\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 3)\n",
    "log = LogisticRegression(solver='liblinear')#regularization is applied by default\n",
    "log.fit(X_train, y_train)\n",
    "accuracy=log.score(X_test,y_test)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "personality=True\n",
    "interaction=True\n",
    "grammar=True\n",
    "if personality==interaction==grammar==True:\n",
    "    print(\"hi\")\n",
    "else:\n",
    "    print(\"No\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def prediction(df, fasttext=False, size=500,\n",
    "               bow=False, top_words=500,\n",
    "               personality=False,\n",
    "               grammar=False, \n",
    "               sentiment=False,\n",
    "               interaction = False,\n",
    "               bow_plus_personality = False,\n",
    "               bow_plus_personality_plus_interactions = False,\n",
    "               bow_plus_personality_plus_grammar=False,\n",
    "               bow_plus_personality_plus_interactions_plus_grammar = False,\n",
    "               seed=1):\n",
    "    '''This function return the accuracy of a prediction dependent on\n",
    "    the set of features used.'''\n",
    "    \n",
    "    # Part I: Creating Feature Set\n",
    "    if bow == True:\n",
    "        # using n-grams as well\n",
    "        cv = CountVectorizer(max_features = top_words, ngram_range=(1,2))\n",
    "        XX = cv.fit_transform(df[\"corpus\"]).toarray()\n",
    "        # using relative word counts instead of absolute ones\n",
    "        tfidf_transformer = TfidfTransformer()\n",
    "        tfidf = tfidf_transformer.fit_transform(XX).toarray()\n",
    "        # tfidf added\n",
    "        X = tfidf\n",
    "\n",
    "\n",
    "    if grammar == True:\n",
    "        # adding Grammar features\n",
    "        grammatical = [\"length\",\"!\",\"?\",\",\",\".\",\" \",'noun', 'adj', 'verb (simple)',\n",
    "       'verb (simple past)', 'verb (past participle)', 'verb (gerund)',\n",
    "       'adverb', 'preposition']\n",
    "        X=df[grammatical]\n",
    "\n",
    "    if personality == True:\n",
    "        # personality added\n",
    "        big5 = [\"Extro\", \"Agree\", \"Neuro\", \"Open\", \"Consc\"]\n",
    "        X=df[big5]\n",
    "\n",
    "  \n",
    "    if interaction == True:\n",
    "        # personality added\n",
    "        interactions=['Extro_Comedy','Extro_Art House & International', 'Extro_Drama','Extro_Action & Adventure', 'Extro_Horror', \\\n",
    "        'Extro_Science Fiction','Extro_Animation', 'Agree_Comedy', 'Agree_Art House & International','Agree_Drama', \\\n",
    "        'Agree_Action & Adventure', 'Agree_Horror','Agree_Science Fiction', 'Agree_Animation', 'Neuro_Comedy',\\\n",
    "        'Neuro_Art House & International', 'Neuro_Drama','Neuro_Action & Adventure', 'Neuro_Horror', 'Neuro_Science Fiction',\\\n",
    "       'Neuro_Animation', 'Open_Comedy', 'Open_Art House & International','Open_Drama', 'Open_Action & Adventure', 'Open_Horror',\\\n",
    "       'Open_Science Fiction', 'Open_Animation', 'Consc_Comedy','Consc_Art House & International', 'Consc_Drama',\\\n",
    "       'Consc_Action & Adventure', 'Consc_Horror', 'Consc_Science Fiction','Consc_Animation']\n",
    "                      \n",
    "        X=df[interactions] \n",
    "\n",
    "    if personality==interaction==grammar==True:\n",
    "        interactions=['Extro_Comedy','Extro_Art House & International', 'Extro_Drama','Extro_Action & Adventure', 'Extro_Horror', \\\n",
    "        'Extro_Science Fiction','Extro_Animation', 'Agree_Comedy', 'Agree_Art House & International','Agree_Drama', \\\n",
    "        'Agree_Action & Adventure', 'Agree_Horror','Agree_Science Fiction', 'Agree_Animation', 'Neuro_Comedy',\\\n",
    "        'Neuro_Art House & International', 'Neuro_Drama','Neuro_Action & Adventure', 'Neuro_Horror', 'Neuro_Science Fiction',\\\n",
    "       'Neuro_Animation', 'Open_Comedy', 'Open_Art House & International','Open_Drama', 'Open_Action & Adventure', 'Open_Horror',\\\n",
    "       'Open_Science Fiction', 'Open_Animation', 'Consc_Comedy','Consc_Art House & International', 'Consc_Drama',\\\n",
    "       'Consc_Action & Adventure', 'Consc_Horror', 'Consc_Science Fiction','Consc_Animation']        \n",
    "        big5 = [\"Extro\", \"Agree\", \"Neuro\", \"Open\", \"Consc\"]\n",
    "        grammatical = [\"length\",\"!\",\"?\",\",\",\".\",\" \",'noun', 'adj', 'verb (simple)',\n",
    "       'verb (simple past)', 'verb (past participle)', 'verb (gerund)',\n",
    "       'adverb', 'preposition']\n",
    "        combined=interactions+big5+grammatical\n",
    "        X=df[combined]\n",
    "\n",
    "    if fasttext==True:\n",
    "        if size==500:\n",
    "            dimensions=list(range(size))\n",
    "            embedding = [str(i) for i in dimensions]\n",
    "        else:\n",
    "            dimensions=list(range(1000,1000+size))\n",
    "            embedding = [str(i) for i in dimensions]\n",
    "            \n",
    "        if personality==True:\n",
    "            big5 = [\"Extro\", \"Agree\", \"Neuro\", \"Open\", \"Consc\"]\n",
    "            embedding = big5 + embedding\n",
    "\n",
    "            \n",
    "        if grammar==True:\n",
    "            grammatical = [\"length\",\"!\",\"?\",\",\",\".\",\" \",'noun', 'adj', 'verb (simple)',\n",
    "       'verb (simple past)', 'verb (past participle)', 'verb (gerund)',\n",
    "       'adverb', 'preposition']\n",
    "            embedding = grammatical + embedding\n",
    "    \n",
    "        if interaction==True:\n",
    "            interactions=['Extro_Comedy','Extro_Art House & International', 'Extro_Drama','Extro_Action & Adventure', 'Extro_Horror', \\\n",
    "        'Extro_Science Fiction','Extro_Animation', 'Agree_Comedy', 'Agree_Art House & International','Agree_Drama', \\\n",
    "        'Agree_Action & Adventure', 'Agree_Horror','Agree_Science Fiction', 'Agree_Animation', 'Neuro_Comedy',\\\n",
    "        'Neuro_Art House & International', 'Neuro_Drama','Neuro_Action & Adventure', 'Neuro_Horror', 'Neuro_Science Fiction',\\\n",
    "       'Neuro_Animation', 'Open_Comedy', 'Open_Art House & International','Open_Drama', 'Open_Action & Adventure', 'Open_Horror',\\\n",
    "       'Open_Science Fiction', 'Open_Animation', 'Consc_Comedy','Consc_Art House & International', 'Consc_Drama',\\\n",
    "       'Consc_Action & Adventure', 'Consc_Horror', 'Consc_Science Fiction','Consc_Animation']\n",
    "            embedding = interactions + embedding\n",
    "            \n",
    "        X=df[embedding]\n",
    "        \n",
    "        print(X.shape)\n",
    "   \n",
    "    if sentiment == True:\n",
    "        X=np.array(df[\"sentiment\"]).reshape(-1, 1)\n",
    "  \n",
    "    if bow_plus_personality == True:\n",
    "        vectorizer = CountVectorizer(binary=True)\n",
    "        X = sp.sparse.hstack((vectorizer.fit_transform(df.reviewText),\n",
    "                      df[['Extro', 'Agree', 'Neuro', 'Open', 'Consc']].values),\n",
    "                          format='csr')\n",
    " \n",
    "    if bow_plus_personality_plus_interactions == True:  \n",
    "        vectorizer = CountVectorizer(binary=True)\n",
    "        X = sp.sparse.hstack((vectorizer.fit_transform(df.reviewText),\n",
    "                      df[['Extro_Comedy',\n",
    "           'Extro_Art House & International', 'Extro_Drama',\n",
    "           'Extro_Action & Adventure', 'Extro_Horror', 'Extro_Science Fiction',\n",
    "           'Extro_Animation', 'Agree_Comedy', 'Agree_Art House & International',\n",
    "           'Agree_Drama', 'Agree_Action & Adventure', 'Agree_Horror',\n",
    "           'Agree_Science Fiction', 'Agree_Animation', 'Neuro_Comedy',\n",
    "           'Neuro_Art House & International', 'Neuro_Drama',\n",
    "           'Neuro_Action & Adventure', 'Neuro_Horror', 'Neuro_Science Fiction',\n",
    "           'Neuro_Animation', 'Open_Comedy', 'Open_Art House & International',\n",
    "           'Open_Drama', 'Open_Action & Adventure', 'Open_Horror',\n",
    "           'Open_Science Fiction', 'Open_Animation', 'Consc_Comedy',\n",
    "           'Consc_Art House & International', 'Consc_Drama',\n",
    "           'Consc_Action & Adventure', 'Consc_Horror', 'Consc_Science Fiction',\n",
    "           'Consc_Animation']].values),\n",
    "                          format='csr')\n",
    "        \n",
    "    if bow_plus_personality_plus_grammar == True:\n",
    "        vectorizer = CountVectorizer(binary=True)\n",
    "        X = sp.sparse.hstack((vectorizer.fit_transform(df.reviewText),\n",
    "                      df[['length', '!', '?', ',', '.', ' ','noun', 'adj', 'verb (simple)',\n",
    "       'verb (simple past)', 'verb (past participle)', 'verb (gerund)',\n",
    "       'adverb', 'preposition','Extro', 'Agree', 'Neuro', 'Open', 'Consc']].values),\n",
    "                          format='csr')\n",
    "        \n",
    "    if bow_plus_personality_plus_interactions_plus_grammar == True:\n",
    "        vectorizer = CountVectorizer(binary=True)\n",
    "        X = sp.sparse.hstack((vectorizer.fit_transform(df.reviewText),\n",
    "                      df[['length', '!', '?', ',', '.', ' ','noun', 'adj', 'verb (simple)',\n",
    "       'verb (simple past)', 'verb (past participle)', 'verb (gerund)',\n",
    "       'adverb', 'preposition','Extro', 'Agree', 'Neuro', 'Open', 'Consc', 'Extro_Comedy',\n",
    "       'Extro_Art House & International', 'Extro_Drama',\n",
    "       'Extro_Action & Adventure', 'Extro_Horror', 'Extro_Science Fiction',\n",
    "       'Extro_Animation', 'Agree_Comedy', 'Agree_Art House & International',\n",
    "       'Agree_Drama', 'Agree_Action & Adventure', 'Agree_Horror',\n",
    "       'Agree_Science Fiction', 'Agree_Animation', 'Neuro_Comedy',\n",
    "       'Neuro_Art House & International', 'Neuro_Drama',\n",
    "       'Neuro_Action & Adventure', 'Neuro_Horror', 'Neuro_Science Fiction',\n",
    "       'Neuro_Animation', 'Open_Comedy', 'Open_Art House & International',\n",
    "       'Open_Drama', 'Open_Action & Adventure', 'Open_Horror',\n",
    "       'Open_Science Fiction', 'Open_Animation', 'Consc_Comedy',\n",
    "       'Consc_Art House & International', 'Consc_Drama',\n",
    "       'Consc_Action & Adventure', 'Consc_Horror', 'Consc_Science Fiction',\n",
    "       'Consc_Animation']].values),\n",
    "                          format='csr')\n",
    "    \n",
    "    # Part II: Running a logit classification on the corresponding feature set X\n",
    "    y=df[\"good\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = seed)\n",
    "    log = LogisticRegression(solver='liblinear')#regularization is applied by default\n",
    "    log.fit(X_train, y_train)\n",
    "    accuracy=log.score(X_test,y_test)\n",
    "    #y_pred = log.predict(X_test)\n",
    "    #cm = confusion_matrix(y_test, y_pred)\n",
    "    #print(cm)\n",
    "    \n",
    "    return(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add word types and sentiment later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>SD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Bag-of-Words_500</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bag-of-Words_1000</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fastText</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Personality</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Grammar</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentiment</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Interaction</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Personality+Grammar+Interaction</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BOW+Personality</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BOW+Personality+Interactions</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BOW+Personality+Interactions+Grammar</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BOW+Personality+Grammar</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fasttext+Personality</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fasttext+Personality+Interactions+Grammar</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fasttext+Personality+Interactions</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fasttext+Personality+Grammar</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Mean   SD\n",
       "Bag-of-Words_500                           NaN  NaN\n",
       "Bag-of-Words_1000                          NaN  NaN\n",
       "fastText                                   NaN  NaN\n",
       "Personality                                NaN  NaN\n",
       "Grammar                                    NaN  NaN\n",
       "Sentiment                                  NaN  NaN\n",
       "Interaction                                NaN  NaN\n",
       "Personality+Grammar+Interaction            NaN  NaN\n",
       "BOW+Personality                            NaN  NaN\n",
       "BOW+Personality+Interactions               NaN  NaN\n",
       "BOW+Personality+Interactions+Grammar       NaN  NaN\n",
       "BOW+Personality+Grammar                    NaN  NaN\n",
       "fasttext+Personality                       NaN  NaN\n",
       "fasttext+Personality+Interactions+Grammar  NaN  NaN\n",
       "fasttext+Personality+Interactions          NaN  NaN\n",
       "fasttext+Personality+Grammar               NaN  NaN"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns=[\"Mean\", \"SD\"]\n",
    "tested=[\"Bag-of-Words_500\",\n",
    "        \"Bag-of-Words_1000\",\n",
    "        \"fastText\",\n",
    "        \"Personality\",\n",
    "        \"Grammar\",\n",
    "        \"Sentiment\",\n",
    "        \"Interaction\",\n",
    "        \"Personality+Grammar+Interaction\",\n",
    "        \"BOW+Personality\",\n",
    "        \"BOW+Personality+Interactions\",\n",
    "        \"BOW+Personality+Interactions+Grammar\",\n",
    "        \"BOW+Personality+Grammar\",\n",
    "        \"fasttext+Personality\",\n",
    "        \"fasttext+Personality+Interactions+Grammar\",\n",
    "        \"fasttext+Personality+Interactions\",\n",
    "        \"fasttext+Personality+Grammar\"\n",
    "       ]\n",
    "result_df=pd.DataFrame(columns=columns, index=tested)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "def create_results(results,n):\n",
    "    '''This functions shows the mean accuracy over n samples.'''\n",
    "    df=x_m\n",
    "    \n",
    "    # 1 BoW\n",
    "    acc=[]\n",
    "    for seed in range(n):\n",
    "        acc_n=prediction(df, bow=True,top_words=500,seed=seed)\n",
    "        acc.append(acc_n)\n",
    "    mean = sum(acc)/len(acc)\n",
    "    sd = statistics.stdev(acc)\n",
    "    results.loc[\"Bag-of-Words_500\",\"Mean\"] = mean\n",
    "    results.loc[\"Bag-of-Words_500\",\"SD\"] = sd\n",
    "\n",
    "    # 2 Tfidf-1000\n",
    "    acc=[]\n",
    "    for seed in range(n):\n",
    "        acc_n=prediction(df, bow=True,top_words=1000,seed=seed)\n",
    "        acc.append(acc_n)\n",
    "    mean = sum(acc)/len(acc)\n",
    "    sd = statistics.stdev(acc)\n",
    "    results.loc[\"Bag-of-Words_1000\",\"Mean\"] = mean\n",
    "    results.loc[\"Bag-of-Words_1000\",\"SD\"] = sd\n",
    "\n",
    "    # 3 fasttext-500\n",
    "    acc=[]\n",
    "    for seed in range(n):\n",
    "        acc_n=prediction(df, fasttext=True,size=500,seed=seed)\n",
    "        acc.append(acc_n)\n",
    "    mean = sum(acc)/len(acc)\n",
    "    sd = statistics.stdev(acc)\n",
    "    results.loc[\"fastText\",\"Mean\"] = mean\n",
    "    results.loc[\"fastText\",\"SD\"] = sd\n",
    "\n",
    "    # 4 personality\n",
    "    acc=[]\n",
    "    for seed in range(n):\n",
    "        acc_n=prediction(df, personality=True,seed=seed)\n",
    "        acc.append(acc_n)\n",
    "    mean = sum(acc)/len(acc)\n",
    "    sd = statistics.stdev(acc)\n",
    "    results.loc[\"Personality\",\"Mean\"] = mean\n",
    "    results.loc[\"Personality\",\"SD\"] = sd\n",
    "\n",
    " \n",
    "    # 5 Personality genre interaction\n",
    "    acc=[]\n",
    "    for seed in range(n):\n",
    "        acc_n=prediction(df, interaction=True,seed=seed)\n",
    "        acc.append(acc_n)\n",
    "    mean = sum(acc)/len(acc)\n",
    "    sd = statistics.stdev(acc)\n",
    "    results.loc[\"Interaction\",\"Mean\"] = mean\n",
    "    results.loc[\"Interaction\",\"SD\"] = sd\n",
    "\n",
    "    # 6 Grammar\n",
    "    acc=[]\n",
    "    for seed in range(n):\n",
    "        acc_n=prediction(df, grammar=True,seed=seed)\n",
    "        acc.append(acc_n)\n",
    "    mean = sum(acc)/len(acc)\n",
    "    sd = statistics.stdev(acc)\n",
    "    results.loc[\"Grammar\",\"Mean\"] = mean\n",
    "    results.loc[\"Grammar\",\"SD\"] = sd\n",
    "    \n",
    "    acc=[]\n",
    "    for seed in range(n):\n",
    "        acc_n=prediction(df, personality=True,interaction=True,grammar=True,seed=seed)\n",
    "        acc.append(acc_n)\n",
    "    mean = sum(acc)/len(acc)\n",
    "    sd = statistics.stdev(acc)\n",
    "    results.loc[\"Personality+Grammar+Interaction\",\"Mean\"] = mean\n",
    "    results.loc[\"Personality+Grammar+Interaction\",\"SD\"] = sd    \n",
    "    \n",
    "    \n",
    "    # 7 BOW_plus_personality\n",
    "       \n",
    "    \n",
    "    acc=[] \n",
    "    for seed in range(n):\n",
    "        acc_n = prediction(df, bow_plus_personality = True ,seed=seed)\n",
    "        acc.append(acc_n)\n",
    "    mean = sum(acc)/len(acc)\n",
    "    sd = statistics.stdev(acc)\n",
    "    results.loc[\"BOW+Personality\",\"Mean\"] = mean\n",
    "    results.loc[\"BOW+Personality\",\"SD\"] = sd\n",
    "    \n",
    "     # 8 BOW_plus_personality_plus_interactions\n",
    "        \n",
    "    acc=[] \n",
    "    for seed in range(n):\n",
    "        acc_n = prediction(df, bow_plus_personality_plus_interactions = True ,seed=seed)\n",
    "        acc.append(acc_n)\n",
    "    mean = sum(acc)/len(acc)\n",
    "    sd = statistics.stdev(acc)\n",
    "    results.loc[ \"BOW+Personality+Interactions\",\"Mean\"] = mean\n",
    "    results.loc[ \"BOW+Personality+Interactions\",\"SD\"] = sd\n",
    "    \n",
    "    # 9 BOW_plus_personality_plus_interactions_plus_grammar\n",
    "        \n",
    "    acc=[] \n",
    "    for seed in range(n):\n",
    "        acc_n = prediction(df, bow_plus_personality_plus_grammar = True ,seed=seed)\n",
    "        acc.append(acc_n)\n",
    "    mean = sum(acc)/len(acc)\n",
    "    sd = statistics.stdev(acc)\n",
    "    results.loc[ \"BOW+Personality+Grammar\",\"Mean\"] = mean\n",
    "    results.loc[ \"BOW+Personality+Grammar\",\"SD\"] = sd\n",
    "    \n",
    "    acc=[] \n",
    "    for seed in range(n):\n",
    "        acc_n = prediction(df, bow_plus_personality_plus_interactions_plus_grammar = True ,seed=seed)\n",
    "        acc.append(acc_n)\n",
    "    mean = sum(acc)/len(acc)\n",
    "    sd = statistics.stdev(acc)\n",
    "    results.loc[ \"BOW+Personality+Interactions+Grammar\",\"Mean\"] = mean\n",
    "    results.loc[ \"BOW+Personality+Interactions+Grammar\",\"SD\"] = sd\n",
    "    \n",
    "    # 10 VADER sentiment\n",
    "    acc=[] \n",
    "    for seed in range(n):\n",
    "        acc_n = prediction(df, sentiment = True ,seed=seed)\n",
    "        acc.append(acc_n)\n",
    "    mean = sum(acc)/len(acc)\n",
    "    sd = statistics.stdev(acc)\n",
    "    results.loc[ \"Sentiment\",\"Mean\"] = mean\n",
    "    results.loc[ \"Sentiment\",\"SD\"] = sd\n",
    "    \n",
    "    \n",
    "    # 11 fasttext_plus_personality\n",
    "       \n",
    "    acc=[] \n",
    "    for seed in range(n):\n",
    "        acc_n = prediction(df, fasttext = True , personality=True, size=500,\n",
    "               bow=False, top_words=500,\n",
    "               grammar=False, \n",
    "               sentiment=False,\n",
    "               interaction = False,\n",
    "               bow_plus_personality = False,\n",
    "               bow_plus_personality_plus_interactions = False,\n",
    "               bow_plus_personality_plus_grammar=False,\n",
    "               bow_plus_personality_plus_interactions_plus_grammar = False,seed=seed)\n",
    "        acc.append(acc_n)\n",
    "    mean = sum(acc)/len(acc)\n",
    "    sd = statistics.stdev(acc)\n",
    "    results.loc[\"fasttext+Personality\",\"Mean\"] = mean\n",
    "    results.loc[\"fasttext+Personality\",\"SD\"] = sd\n",
    "    \n",
    "     # 12 BOW_plus_personality_plus_interactions\n",
    "        \n",
    "    acc=[] \n",
    "    for seed in range(n):\n",
    "        acc_n = prediction(df, fasttext = True ,personality=True, interaction=True,size=500,\n",
    "               bow=False, top_words=500,\n",
    "               grammar=False, \n",
    "               sentiment=False,\n",
    "               bow_plus_personality = False,\n",
    "               bow_plus_personality_plus_interactions = False,\n",
    "               bow_plus_personality_plus_grammar=False,\n",
    "               bow_plus_personality_plus_interactions_plus_grammar = False,seed=seed)\n",
    "        acc.append(acc_n)\n",
    "    mean = sum(acc)/len(acc)\n",
    "    sd = statistics.stdev(acc)\n",
    "    results.loc[ \"fasttext+Personality+Interactions\",\"Mean\"] = mean\n",
    "    results.loc[ \"fasttext+Personality+Interactions\",\"SD\"] = sd\n",
    "    \n",
    "    # 9 BOW_plus_personality_plus_interactions_plus_grammar\n",
    "        \n",
    "    acc=[] \n",
    "    for seed in range(n):\n",
    "        acc_n = prediction(df, fasttext = True ,personality=True,grammar=True, size=500,\n",
    "               bow=False, top_words=500,\n",
    "               sentiment=False,\n",
    "               interaction = False,\n",
    "               bow_plus_personality = False,\n",
    "               bow_plus_personality_plus_interactions = False,\n",
    "               bow_plus_personality_plus_grammar=False,\n",
    "               bow_plus_personality_plus_interactions_plus_grammar = False,seed=seed)\n",
    "        acc.append(acc_n)\n",
    "    mean = sum(acc)/len(acc)\n",
    "    sd = statistics.stdev(acc)\n",
    "    results.loc[ \"fasttext+Personality+Grammar\",\"Mean\"] = mean\n",
    "    results.loc[ \"fasttext+Personality+Grammar\",\"SD\"] = sd\n",
    "    \n",
    "    acc=[] \n",
    "    for seed in range(n):\n",
    "        acc_n = prediction(df, fasttext = True ,personality=True,interaction=True, size=500,\n",
    "               bow=False, top_words=500,\n",
    "               grammar=True, \n",
    "               sentiment=False,\n",
    "               bow_plus_personality = False,\n",
    "               bow_plus_personality_plus_interactions = False,\n",
    "               bow_plus_personality_plus_grammar=False,\n",
    "               bow_plus_personality_plus_interactions_plus_grammar = False,seed=seed)\n",
    "        acc.append(acc_n)\n",
    "    mean = sum(acc)/len(acc)\n",
    "    sd = statistics.stdev(acc)\n",
    "    results.loc[ \"fasttext+Personality+Interactions+Grammar\",\"Mean\"] = mean\n",
    "    results.loc[ \"fasttext+Personality+Interactions+Grammar\",\"SD\"] = sd\n",
    "        \n",
    "    \n",
    "    return(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18808, 500)\n",
      "(18808, 500)\n",
      "(18808, 505)\n",
      "(18808, 505)\n",
      "(18808, 540)\n",
      "(18808, 540)\n",
      "(18808, 519)\n",
      "(18808, 519)\n",
      "(18808, 554)\n",
      "(18808, 554)\n"
     ]
    }
   ],
   "source": [
    "resu=create_results(result_df,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>SD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Bag-of-Words_500</th>\n",
       "      <td>0.813264</td>\n",
       "      <td>0.00206756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bag-of-Words_1000</th>\n",
       "      <td>0.836124</td>\n",
       "      <td>0.00319533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fastText</th>\n",
       "      <td>0.830409</td>\n",
       "      <td>0.00300737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Personality</th>\n",
       "      <td>0.623206</td>\n",
       "      <td>0.00958598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Grammar</th>\n",
       "      <td>0.611244</td>\n",
       "      <td>0.00770637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentiment</th>\n",
       "      <td>0.703349</td>\n",
       "      <td>0.014285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Interaction</th>\n",
       "      <td>0.622807</td>\n",
       "      <td>0.0018796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Personality+Grammar+Interaction</th>\n",
       "      <td>0.673312</td>\n",
       "      <td>0.00150368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BOW+Personality</th>\n",
       "      <td>0.85513</td>\n",
       "      <td>0.00639065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BOW+Personality+Interactions</th>\n",
       "      <td>0.855263</td>\n",
       "      <td>0.00357125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BOW+Personality+Interactions+Grammar</th>\n",
       "      <td>0.853535</td>\n",
       "      <td>0.00150368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BOW+Personality+Grammar</th>\n",
       "      <td>0.855529</td>\n",
       "      <td>0.00469901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fasttext+Personality</th>\n",
       "      <td>0.830941</td>\n",
       "      <td>0.00112776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fasttext+Personality+Interactions+Grammar</th>\n",
       "      <td>0.835593</td>\n",
       "      <td>0.000563881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fasttext+Personality+Interactions</th>\n",
       "      <td>0.829878</td>\n",
       "      <td>0.0018796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fasttext+Personality+Grammar</th>\n",
       "      <td>0.836257</td>\n",
       "      <td>0.00225552</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Mean           SD\n",
       "Bag-of-Words_500                           0.813264   0.00206756\n",
       "Bag-of-Words_1000                          0.836124   0.00319533\n",
       "fastText                                   0.830409   0.00300737\n",
       "Personality                                0.623206   0.00958598\n",
       "Grammar                                    0.611244   0.00770637\n",
       "Sentiment                                  0.703349     0.014285\n",
       "Interaction                                0.622807    0.0018796\n",
       "Personality+Grammar+Interaction            0.673312   0.00150368\n",
       "BOW+Personality                             0.85513   0.00639065\n",
       "BOW+Personality+Interactions               0.855263   0.00357125\n",
       "BOW+Personality+Interactions+Grammar       0.853535   0.00150368\n",
       "BOW+Personality+Grammar                    0.855529   0.00469901\n",
       "fasttext+Personality                       0.830941   0.00112776\n",
       "fasttext+Personality+Interactions+Grammar  0.835593  0.000563881\n",
       "fasttext+Personality+Interactions          0.829878    0.0018796\n",
       "fasttext+Personality+Grammar               0.836257   0.00225552"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>SD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Bag-of-Words_500</th>\n",
       "      <td>0.813264</td>\n",
       "      <td>0.00206756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bag-of-Words_1000</th>\n",
       "      <td>0.836124</td>\n",
       "      <td>0.00319533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fastText</th>\n",
       "      <td>0.830409</td>\n",
       "      <td>0.00300737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Personality</th>\n",
       "      <td>0.623206</td>\n",
       "      <td>0.00958598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Grammar</th>\n",
       "      <td>0.611244</td>\n",
       "      <td>0.00770637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentiment</th>\n",
       "      <td>0.703349</td>\n",
       "      <td>0.014285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Interaction</th>\n",
       "      <td>0.622807</td>\n",
       "      <td>0.0018796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BOW+Personality</th>\n",
       "      <td>0.85513</td>\n",
       "      <td>0.00639065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BOW+Personality+Interactions</th>\n",
       "      <td>0.855263</td>\n",
       "      <td>0.00357125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BOW+Personality+Interactions+Grammar</th>\n",
       "      <td>0.853535</td>\n",
       "      <td>0.00150368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BOW+Personality+Grammar</th>\n",
       "      <td>0.855529</td>\n",
       "      <td>0.00469901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fasttext+Personality</th>\n",
       "      <td>0.830941</td>\n",
       "      <td>0.00112776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fasttext+Personality+Interactions+Grammar</th>\n",
       "      <td>0.835593</td>\n",
       "      <td>0.000563881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fasttext+Personality+Interactions</th>\n",
       "      <td>0.829878</td>\n",
       "      <td>0.0018796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fasttext+Personality+Grammar</th>\n",
       "      <td>0.836257</td>\n",
       "      <td>0.00225552</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Mean           SD\n",
       "Bag-of-Words_500                           0.813264   0.00206756\n",
       "Bag-of-Words_1000                          0.836124   0.00319533\n",
       "fastText                                   0.830409   0.00300737\n",
       "Personality                                0.623206   0.00958598\n",
       "Grammar                                    0.611244   0.00770637\n",
       "Sentiment                                  0.703349     0.014285\n",
       "Interaction                                0.622807    0.0018796\n",
       "BOW+Personality                             0.85513   0.00639065\n",
       "BOW+Personality+Interactions               0.855263   0.00357125\n",
       "BOW+Personality+Interactions+Grammar       0.853535   0.00150368\n",
       "BOW+Personality+Grammar                    0.855529   0.00469901\n",
       "fasttext+Personality                       0.830941   0.00112776\n",
       "fasttext+Personality+Interactions+Grammar  0.835593  0.000563881\n",
       "fasttext+Personality+Interactions          0.829878    0.0018796\n",
       "fasttext+Personality+Grammar               0.836257   0.00225552"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resu25 = create_results(result_df,25)\n",
    "resu25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>SD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Bag-of-Words_500</th>\n",
       "      <td>0.818905</td>\n",
       "      <td>0.0053227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bag-of-Words_1000</th>\n",
       "      <td>0.84033</td>\n",
       "      <td>0.00486083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fastText</th>\n",
       "      <td>0.83512</td>\n",
       "      <td>0.00518655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Personality</th>\n",
       "      <td>0.62622</td>\n",
       "      <td>0.00786619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Grammar</th>\n",
       "      <td>0.562158</td>\n",
       "      <td>0.00701048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentiment</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Interaction</th>\n",
       "      <td>0.622446</td>\n",
       "      <td>0.00522859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BOW+Personality</th>\n",
       "      <td>0.857682</td>\n",
       "      <td>0.00586806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BOW+Personality+Interactions</th>\n",
       "      <td>0.856534</td>\n",
       "      <td>0.00554962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BOW+Personality+Interactions+Grammar</th>\n",
       "      <td>0.857905</td>\n",
       "      <td>0.00554567</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Mean          SD\n",
       "Bag-of-Words_500                      0.818905   0.0053227\n",
       "Bag-of-Words_1000                      0.84033  0.00486083\n",
       "fastText                               0.83512  0.00518655\n",
       "Personality                            0.62622  0.00786619\n",
       "Grammar                               0.562158  0.00701048\n",
       "Sentiment                                  NaN         NaN\n",
       "Interaction                           0.622446  0.00522859\n",
       "BOW+Personality                       0.857682  0.00586806\n",
       "BOW+Personality+Interactions          0.856534  0.00554962\n",
       "BOW+Personality+Interactions+Grammar  0.857905  0.00554567"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resu25.to_excel(\"results_07_08.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#will be used for tomorrow\n",
    "# adding word types and sentiment VADER\n",
    "    # 8 Sentiment\n",
    "    acc=[]\n",
    "    for seed in range(n):\n",
    "        acc_n=prediction(df, sentiment=True,seed=seed)\n",
    "        acc.append(acc_n)\n",
    "    mean = sum(acc)/len(acc)\n",
    "    sd = statistics.stdev(acc)\n",
    "    results.loc[\"Sentiment\",\"Mean\"] = mean\n",
    "    results.loc[\"Sentiment\",\"SD\"] = sd\n",
    "    \n",
    "# with grammar\n",
    "        acc=[]\n",
    "        for seed in range(n):\n",
    "            acc_n=prediction(df, tfidf=True,top_words=500,grammar=True,seed=seed)\n",
    "            acc.append(acc_n)\n",
    "        mean = sum(acc)/len(acc)\n",
    "        sd = statistics.stdev(acc)\n",
    "        results.loc[\"Tfidf+Grammar\",area+\": Mean\"] = mean\n",
    "        results.loc[\"Tfidf+Grammar\",area+\": SD\"] = sd\n",
    "\n",
    "        # with sentiment\n",
    "        acc=[]\n",
    "        for seed in range(n):\n",
    "            acc_n=prediction(df, tfidf=True,top_words=500,sentiment=True,seed=seed)\n",
    "            acc.append(acc_n)\n",
    "        mean = sum(acc)/len(acc)\n",
    "        sd = statistics.stdev(acc)\n",
    "        results.loc[\"Tfidf+Sentiment\",area+\": Mean\"] = mean\n",
    "        results.loc[\"Tfidf+Sentiment\",area+\": SD\"] = sd\n",
    "\n",
    "\n",
    "        # Fasttext with combination\n",
    "        acc=[]\n",
    "        for seed in range(n):\n",
    "            acc_n=prediction(df, fasttext=True,size=500,personality=True,seed=seed)\n",
    "            acc.append(acc_n)\n",
    "        mean = sum(acc)/len(acc)\n",
    "        sd = statistics.stdev(acc)\n",
    "        results.loc[\"fastText+Personality\",area+\": Mean\"] = mean\n",
    "        results.loc[\"fastText+Personality\",area+\": SD\"] = sd\n",
    "        \n",
    "        acc=[]\n",
    "        for seed in range(n):\n",
    "            acc_n=prediction(df, fasttext=True,size=500,grammar=True,seed=seed)\n",
    "            acc.append(acc_n)\n",
    "        mean = sum(acc)/len(acc)\n",
    "        sd = statistics.stdev(acc)\n",
    "        results.loc[\"fastText+Grammar\",area+\": Mean\"] = mean\n",
    "        results.loc[\"fastText+Grammar\",area+\": SD\"] = sd        \n",
    "        \n",
    "        acc=[]\n",
    "        for seed in range(n):\n",
    "            acc_n=prediction(df, fasttext=True,size=500,sentiment=True,seed=seed)\n",
    "            acc.append(acc_n)\n",
    "        mean = sum(acc)/len(acc)\n",
    "        sd = statistics.stdev(acc)\n",
    "        results.loc[\"fastText+Sentiment\",area+\": Mean\"] = mean\n",
    "        results.loc[\"fastText+Sentiment\",area+\": SD\"] = sd\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
