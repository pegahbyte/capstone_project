{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "- fasttext Features are added (can be skipped and pickled file used)\n",
    "- Functions that run n multiple predictions for seperate models (SVM, NB, Logit) on the dataset.\n",
    "- using different sets of features\n",
    "- returning the mean and variance of accuracy over the n runs.\n",
    "- results are exported as Excel Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\MAHAM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#importing libraries for NLP\n",
    "import pandas as pd\n",
    "import os as os\n",
    "import json as js\n",
    "import gzip\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "%matplotlib inline\n",
    "\n",
    "#Image processing material\n",
    "import imageio\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from scipy.special import expit as sigmoid\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "#importing libraries for NLP\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from collections import Counter\n",
    "\n",
    "#Text Classification Material\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.fasttext import FastText as FT_gensim\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models import FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe\n",
    "Use the pickled full-feature dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('df_07_08', 'rb') as fr:\n",
    "    df = pickle.load(fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>good</th>\n",
       "      <th>Comedy</th>\n",
       "      <th>Art House &amp; International</th>\n",
       "      <th>Drama</th>\n",
       "      <th>Action &amp; Adventure</th>\n",
       "      <th>Horror</th>\n",
       "      <th>...</th>\n",
       "      <th>corpus</th>\n",
       "      <th>noun</th>\n",
       "      <th>adj</th>\n",
       "      <th>verb (simple)</th>\n",
       "      <th>verb (simple past)</th>\n",
       "      <th>verb (past participle)</th>\n",
       "      <th>verb (gerund)</th>\n",
       "      <th>adverb</th>\n",
       "      <th>preposition</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A19JYLHD94K94D</td>\n",
       "      <td>6304239327</td>\n",
       "      <td>Jan Svankajer's feature film follow up to his ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>jan svankaj featur film follow masterpiec alic...</td>\n",
       "      <td>0.589624</td>\n",
       "      <td>0.801433</td>\n",
       "      <td>-1.084828</td>\n",
       "      <td>-0.866063</td>\n",
       "      <td>-1.115746</td>\n",
       "      <td>0.020607</td>\n",
       "      <td>0.578095</td>\n",
       "      <td>0.141700</td>\n",
       "      <td>0.9904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A144W50UYZG4YX</td>\n",
       "      <td>630339406X</td>\n",
       "      <td>Big-1988100mins/ColourTom HanksElizabeth Perki...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>big min colourtom hankselizabeth perkinsrobert...</td>\n",
       "      <td>0.923381</td>\n",
       "      <td>1.265269</td>\n",
       "      <td>-1.372903</td>\n",
       "      <td>-0.648728</td>\n",
       "      <td>0.800474</td>\n",
       "      <td>0.674204</td>\n",
       "      <td>-0.962428</td>\n",
       "      <td>0.004977</td>\n",
       "      <td>0.9934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A16QODENBJVUI1</td>\n",
       "      <td>B0007PALGG</td>\n",
       "      <td>Of all the films coming out this holiday movie...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>film come holiday movi season kinsey near top ...</td>\n",
       "      <td>0.789278</td>\n",
       "      <td>0.328968</td>\n",
       "      <td>-0.437079</td>\n",
       "      <td>0.246639</td>\n",
       "      <td>-0.340824</td>\n",
       "      <td>0.149929</td>\n",
       "      <td>-0.029449</td>\n",
       "      <td>1.596602</td>\n",
       "      <td>0.9962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A3L2856DIMUXJY</td>\n",
       "      <td>B0001DCR0C</td>\n",
       "      <td>As noted by a number of reviewers, this starte...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>note number review start life play director ma...</td>\n",
       "      <td>0.210671</td>\n",
       "      <td>-0.553402</td>\n",
       "      <td>0.253301</td>\n",
       "      <td>0.445873</td>\n",
       "      <td>0.343758</td>\n",
       "      <td>0.523410</td>\n",
       "      <td>-1.190013</td>\n",
       "      <td>1.242949</td>\n",
       "      <td>0.9455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A1Z54EM24Y40LL</td>\n",
       "      <td>B00004WI5C</td>\n",
       "      <td>This is really a cute video.  It's pretty long...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>realli cute video pretti long though may hard ...</td>\n",
       "      <td>-1.755248</td>\n",
       "      <td>-0.521771</td>\n",
       "      <td>1.846551</td>\n",
       "      <td>-1.006187</td>\n",
       "      <td>-1.115746</td>\n",
       "      <td>-0.137556</td>\n",
       "      <td>1.073756</td>\n",
       "      <td>0.138101</td>\n",
       "      <td>0.9606</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 68 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin  \\\n",
       "0  A19JYLHD94K94D  6304239327   \n",
       "1  A144W50UYZG4YX  630339406X   \n",
       "2  A16QODENBJVUI1  B0007PALGG   \n",
       "3  A3L2856DIMUXJY  B0001DCR0C   \n",
       "4  A1Z54EM24Y40LL  B00004WI5C   \n",
       "\n",
       "                                          reviewText  overall  good  Comedy  \\\n",
       "0  Jan Svankajer's feature film follow up to his ...      5.0     1       0   \n",
       "1  Big-1988100mins/ColourTom HanksElizabeth Perki...      4.0     1       0   \n",
       "2  Of all the films coming out this holiday movie...      4.0     1       0   \n",
       "3  As noted by a number of reviewers, this starte...      4.0     1       0   \n",
       "4  This is really a cute video.  It's pretty long...      4.0     1       0   \n",
       "\n",
       "   Art House & International  Drama  Action & Adventure  Horror  ...  \\\n",
       "0                          1      1                   0       0  ...   \n",
       "1                          0      0                   0       0  ...   \n",
       "2                          0      1                   0       0  ...   \n",
       "3                          1      1                   0       0  ...   \n",
       "4                          0      0                   0       0  ...   \n",
       "\n",
       "                                              corpus      noun       adj  \\\n",
       "0  jan svankaj featur film follow masterpiec alic...  0.589624  0.801433   \n",
       "1  big min colourtom hankselizabeth perkinsrobert...  0.923381  1.265269   \n",
       "2  film come holiday movi season kinsey near top ...  0.789278  0.328968   \n",
       "3  note number review start life play director ma...  0.210671 -0.553402   \n",
       "4  realli cute video pretti long though may hard ... -1.755248 -0.521771   \n",
       "\n",
       "   verb (simple)  verb (simple past)  verb (past participle)  verb (gerund)  \\\n",
       "0      -1.084828           -0.866063               -1.115746       0.020607   \n",
       "1      -1.372903           -0.648728                0.800474       0.674204   \n",
       "2      -0.437079            0.246639               -0.340824       0.149929   \n",
       "3       0.253301            0.445873                0.343758       0.523410   \n",
       "4       1.846551           -1.006187               -1.115746      -0.137556   \n",
       "\n",
       "     adverb  preposition  sentiment  \n",
       "0  0.578095     0.141700     0.9904  \n",
       "1 -0.962428     0.004977     0.9934  \n",
       "2 -0.029449     1.596602     0.9962  \n",
       "3 -1.190013     1.242949     0.9455  \n",
       "4  1.073756     0.138101     0.9606  \n",
       "\n",
       "[5 rows x 68 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentence(df):\n",
    "    df[\"split\"]=df[\"corpus\"].apply(lambda x: x.split())\n",
    "split_sentence(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(df.split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes about 10 min:\n",
    "size=500\n",
    "#Movie and TV Data\n",
    "model_m = FastText(size=size, seed=1)  # instantiate\n",
    "model_m.build_vocab(sentences=sentences)\n",
    "model_m.train(sentences=sentences, total_examples=len(sentences), epochs=30)  # train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_vectorizer(sent, model):\n",
    "    '''Uses a pretrained FastText model to create an avage over \n",
    "    a review.'''\n",
    "    sent_vec =[]\n",
    "    numw = 0\n",
    "    for w in sent:\n",
    "        try:\n",
    "            if numw == 0:\n",
    "                sent_vec = model.wv[w]\n",
    "            else:\n",
    "                sent_vec = np.add(sent_vec, model.wv[w])\n",
    "            numw+=1\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return np.asarray(sent_vec) / numw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_sent(df,model,size):\n",
    "    '''Takes a dataframe and adds features: Every dimension of\n",
    "    a model becomes a column. Return new dataframe'''\n",
    "    df[\"avg_sentence\"]=df[\"split\"].apply(lambda x: sent_vectorizer(x, model))\n",
    "    length=df[\"avg_sentence\"].apply(lambda x: len(x))\n",
    "    df2=df[length==size]\n",
    "    for i in range(size):\n",
    "        if size==500:\n",
    "            df2[str(i)] = df2[\"avg_sentence\"].apply(lambda x : x[i])\n",
    "        else:\n",
    "            df2[str(1000+i)] = df2[\"avg_sentence\"].apply(lambda x : x[i])\n",
    "    return(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding fasttext-500 features to df's\n",
    "x_m=avg_sent(df, model_m, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle full feature df's\n",
    "with open ('df_07_08_fasttext', 'wb') as fw:\n",
    "    pickle.dump(x_m, fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open ('df_07_08_fasttext', 'rb') as fr:\n",
    "    x_m = pickle.load(fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def prediction(df, fasttext=False, size=500,\n",
    "               bow=False, top_words=500,\n",
    "               personality=False,\n",
    "               grammar=False, \n",
    "               sentiment=False,\n",
    "               interaction = False,\n",
    "               bow_plus_personality = False,\n",
    "               bow_plus_personality_plus_interactions = False,\n",
    "               bow_plus_personality_plus_grammar=False,\n",
    "               bow_plus_personality_plus_interactions_plus_grammar = False,\n",
    "               model=\"logit\",\n",
    "               seed=1):\n",
    "    '''This function return the accuracy of a prediction dependent on\n",
    "    the set of features used.'''\n",
    "    \n",
    "    # Part I: Creating Feature Set\n",
    "    if bow == True:\n",
    "        # using n-grams as well\n",
    "        cv = CountVectorizer(max_features = top_words, ngram_range=(1,2))\n",
    "        XX = cv.fit_transform(df[\"corpus\"]).toarray()\n",
    "        # using relative word counts instead of absolute ones\n",
    "        tfidf_transformer = TfidfTransformer()\n",
    "        tfidf = tfidf_transformer.fit_transform(XX).toarray()\n",
    "        # tfidf added\n",
    "        X = tfidf\n",
    "\n",
    "\n",
    "    if grammar == True:\n",
    "        # adding Grammar features\n",
    "        grammatical = [\"length\",\"!\",\"?\",\",\",\".\",\" \",'noun', 'adj', 'verb (simple)',\n",
    "       'verb (simple past)', 'verb (past participle)', 'verb (gerund)',\n",
    "       'adverb', 'preposition']\n",
    "        X=df[grammatical]\n",
    "\n",
    "    if personality == True:\n",
    "        # personality added\n",
    "        big5 = [\"Extro\", \"Agree\", \"Neuro\", \"Open\", \"Consc\"]\n",
    "        X=df[big5]\n",
    "\n",
    "  \n",
    "    if interaction == True:\n",
    "        # personality added\n",
    "        interactions=['Extro_Comedy','Extro_Art House & International', 'Extro_Drama','Extro_Action & Adventure', 'Extro_Horror', \\\n",
    "        'Extro_Science Fiction','Extro_Animation', 'Agree_Comedy', 'Agree_Art House & International','Agree_Drama', \\\n",
    "        'Agree_Action & Adventure', 'Agree_Horror','Agree_Science Fiction', 'Agree_Animation', 'Neuro_Comedy',\\\n",
    "        'Neuro_Art House & International', 'Neuro_Drama','Neuro_Action & Adventure', 'Neuro_Horror', 'Neuro_Science Fiction',\\\n",
    "       'Neuro_Animation', 'Open_Comedy', 'Open_Art House & International','Open_Drama', 'Open_Action & Adventure', 'Open_Horror',\\\n",
    "       'Open_Science Fiction', 'Open_Animation', 'Consc_Comedy','Consc_Art House & International', 'Consc_Drama',\\\n",
    "       'Consc_Action & Adventure', 'Consc_Horror', 'Consc_Science Fiction','Consc_Animation']\n",
    "                      \n",
    "        X=df[interactions] \n",
    "\n",
    "    if personality==interaction==grammar==True:\n",
    "        interactions=['Extro_Comedy','Extro_Art House & International', 'Extro_Drama','Extro_Action & Adventure', 'Extro_Horror', \\\n",
    "        'Extro_Science Fiction','Extro_Animation', 'Agree_Comedy', 'Agree_Art House & International','Agree_Drama', \\\n",
    "        'Agree_Action & Adventure', 'Agree_Horror','Agree_Science Fiction', 'Agree_Animation', 'Neuro_Comedy',\\\n",
    "        'Neuro_Art House & International', 'Neuro_Drama','Neuro_Action & Adventure', 'Neuro_Horror', 'Neuro_Science Fiction',\\\n",
    "       'Neuro_Animation', 'Open_Comedy', 'Open_Art House & International','Open_Drama', 'Open_Action & Adventure', 'Open_Horror',\\\n",
    "       'Open_Science Fiction', 'Open_Animation', 'Consc_Comedy','Consc_Art House & International', 'Consc_Drama',\\\n",
    "       'Consc_Action & Adventure', 'Consc_Horror', 'Consc_Science Fiction','Consc_Animation']        \n",
    "        big5 = [\"Extro\", \"Agree\", \"Neuro\", \"Open\", \"Consc\"]\n",
    "        grammatical = [\"length\",\"!\",\"?\",\",\",\".\",\" \",'noun', 'adj', 'verb (simple)',\n",
    "       'verb (simple past)', 'verb (past participle)', 'verb (gerund)',\n",
    "       'adverb', 'preposition']\n",
    "        combined=interactions+big5+grammatical\n",
    "        X=df[combined]\n",
    "\n",
    "    if fasttext==True:\n",
    "        if size==500:\n",
    "            dimensions=list(range(size))\n",
    "            embedding = [str(i) for i in dimensions]\n",
    "        else:\n",
    "            dimensions=list(range(1000,1000+size))\n",
    "            embedding = [str(i) for i in dimensions]\n",
    "            \n",
    "        if personality==True:\n",
    "            big5 = [\"Extro\", \"Agree\", \"Neuro\", \"Open\", \"Consc\"]\n",
    "            embedding = big5 + embedding\n",
    "\n",
    "            \n",
    "        if grammar==True:\n",
    "            grammatical = [\"length\",\"!\",\"?\",\",\",\".\",\" \",'noun', 'adj', 'verb (simple)',\n",
    "       'verb (simple past)', 'verb (past participle)', 'verb (gerund)',\n",
    "       'adverb', 'preposition']\n",
    "            embedding = grammatical + embedding\n",
    "    \n",
    "        if interaction==True:\n",
    "            interactions=['Extro_Comedy','Extro_Art House & International', 'Extro_Drama','Extro_Action & Adventure', 'Extro_Horror', \\\n",
    "        'Extro_Science Fiction','Extro_Animation', 'Agree_Comedy', 'Agree_Art House & International','Agree_Drama', \\\n",
    "        'Agree_Action & Adventure', 'Agree_Horror','Agree_Science Fiction', 'Agree_Animation', 'Neuro_Comedy',\\\n",
    "        'Neuro_Art House & International', 'Neuro_Drama','Neuro_Action & Adventure', 'Neuro_Horror', 'Neuro_Science Fiction',\\\n",
    "       'Neuro_Animation', 'Open_Comedy', 'Open_Art House & International','Open_Drama', 'Open_Action & Adventure', 'Open_Horror',\\\n",
    "       'Open_Science Fiction', 'Open_Animation', 'Consc_Comedy','Consc_Art House & International', 'Consc_Drama',\\\n",
    "       'Consc_Action & Adventure', 'Consc_Horror', 'Consc_Science Fiction','Consc_Animation']\n",
    "            embedding = interactions + embedding\n",
    "            \n",
    "        X=df[embedding]\n",
    "        \n",
    "        print(X.shape)\n",
    "   \n",
    "    if sentiment == True:\n",
    "        X=np.array(df[\"sentiment\"]).reshape(-1, 1)\n",
    "  \n",
    "    if bow_plus_personality == True:\n",
    "        vectorizer = CountVectorizer(binary=True)\n",
    "        X = sp.sparse.hstack((vectorizer.fit_transform(df.reviewText),\n",
    "                      df[['Extro', 'Agree', 'Neuro', 'Open', 'Consc']].values),\n",
    "                          format='csr')\n",
    "        X=X.toarray()\n",
    " \n",
    "    if bow_plus_personality_plus_interactions == True:  \n",
    "        vectorizer = CountVectorizer(binary=True)\n",
    "        X = sp.sparse.hstack((vectorizer.fit_transform(df.reviewText),\n",
    "                      df[['Extro_Comedy',\n",
    "           'Extro_Art House & International', 'Extro_Drama',\n",
    "           'Extro_Action & Adventure', 'Extro_Horror', 'Extro_Science Fiction',\n",
    "           'Extro_Animation', 'Agree_Comedy', 'Agree_Art House & International',\n",
    "           'Agree_Drama', 'Agree_Action & Adventure', 'Agree_Horror',\n",
    "           'Agree_Science Fiction', 'Agree_Animation', 'Neuro_Comedy',\n",
    "           'Neuro_Art House & International', 'Neuro_Drama',\n",
    "           'Neuro_Action & Adventure', 'Neuro_Horror', 'Neuro_Science Fiction',\n",
    "           'Neuro_Animation', 'Open_Comedy', 'Open_Art House & International',\n",
    "           'Open_Drama', 'Open_Action & Adventure', 'Open_Horror',\n",
    "           'Open_Science Fiction', 'Open_Animation', 'Consc_Comedy',\n",
    "           'Consc_Art House & International', 'Consc_Drama',\n",
    "           'Consc_Action & Adventure', 'Consc_Horror', 'Consc_Science Fiction',\n",
    "           'Consc_Animation']].values),\n",
    "                          format='csr')\n",
    "        \n",
    "    if bow_plus_personality_plus_grammar == True:\n",
    "        vectorizer = CountVectorizer(binary=True)\n",
    "        X = sp.sparse.hstack((vectorizer.fit_transform(df.reviewText),\n",
    "                      df[['length', '!', '?', ',', '.', ' ','noun', 'adj', 'verb (simple)',\n",
    "       'verb (simple past)', 'verb (past participle)', 'verb (gerund)',\n",
    "       'adverb', 'preposition','Extro', 'Agree', 'Neuro', 'Open', 'Consc']].values),\n",
    "                          format='csr')\n",
    "        \n",
    "    if bow_plus_personality_plus_interactions_plus_grammar == True:\n",
    "        vectorizer = CountVectorizer(binary=True)\n",
    "        X = sp.sparse.hstack((vectorizer.fit_transform(df.reviewText),\n",
    "                      df[['length', '!', '?', ',', '.', ' ','noun', 'adj', 'verb (simple)',\n",
    "       'verb (simple past)', 'verb (past participle)', 'verb (gerund)',\n",
    "       'adverb', 'preposition','Extro', 'Agree', 'Neuro', 'Open', 'Consc', 'Extro_Comedy',\n",
    "       'Extro_Art House & International', 'Extro_Drama',\n",
    "       'Extro_Action & Adventure', 'Extro_Horror', 'Extro_Science Fiction',\n",
    "       'Extro_Animation', 'Agree_Comedy', 'Agree_Art House & International',\n",
    "       'Agree_Drama', 'Agree_Action & Adventure', 'Agree_Horror',\n",
    "       'Agree_Science Fiction', 'Agree_Animation', 'Neuro_Comedy',\n",
    "       'Neuro_Art House & International', 'Neuro_Drama',\n",
    "       'Neuro_Action & Adventure', 'Neuro_Horror', 'Neuro_Science Fiction',\n",
    "       'Neuro_Animation', 'Open_Comedy', 'Open_Art House & International',\n",
    "       'Open_Drama', 'Open_Action & Adventure', 'Open_Horror',\n",
    "       'Open_Science Fiction', 'Open_Animation', 'Consc_Comedy',\n",
    "       'Consc_Art House & International', 'Consc_Drama',\n",
    "       'Consc_Action & Adventure', 'Consc_Horror', 'Consc_Science Fiction',\n",
    "       'Consc_Animation']].values),\n",
    "                          format='csr')\n",
    "    \n",
    "    # Part II: Running a logit classification on the corresponding feature set X\n",
    "    y=df[\"good\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = seed)\n",
    "    if model==\"logit\":\n",
    "        log = LogisticRegression(solver='liblinear')#regularization is applied by default\n",
    "    elif model==\"nb\":\n",
    "        log = GaussianNB()\n",
    "        #X = X.todense()\n",
    "        #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = seed)\n",
    "    elif model==\"nn\":\n",
    "        log = MLPClassifier()\n",
    "    elif model==\"svm\":\n",
    "        log = SVC(gamma='auto')\n",
    "\n",
    "    log.fit(X_train, y_train)\n",
    "    accuracy=log.score(X_test,y_test)\n",
    "    #y_pred = log.predict(X_test)\n",
    "    #cm = confusion_matrix(y_test, y_pred)\n",
    "    #print(cm)\n",
    "    \n",
    "    return(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add word types and sentiment later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>SD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Bag-of-Words_500</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bag-of-Words_1000</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fastText</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Personality</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Grammar</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentiment</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Interaction</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Personality+Grammar+Interaction</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BOW+Personality</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BOW+Personality+Interactions</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BOW+Personality+Interactions+Grammar</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BOW+Personality+Grammar</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fasttext+Personality</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fasttext+Personality+Interactions+Grammar</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fasttext+Personality+Interactions</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fasttext+Personality+Grammar</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Mean   SD\n",
       "Bag-of-Words_500                           NaN  NaN\n",
       "Bag-of-Words_1000                          NaN  NaN\n",
       "fastText                                   NaN  NaN\n",
       "Personality                                NaN  NaN\n",
       "Grammar                                    NaN  NaN\n",
       "Sentiment                                  NaN  NaN\n",
       "Interaction                                NaN  NaN\n",
       "Personality+Grammar+Interaction            NaN  NaN\n",
       "BOW+Personality                            NaN  NaN\n",
       "BOW+Personality+Interactions               NaN  NaN\n",
       "BOW+Personality+Interactions+Grammar       NaN  NaN\n",
       "BOW+Personality+Grammar                    NaN  NaN\n",
       "fasttext+Personality                       NaN  NaN\n",
       "fasttext+Personality+Interactions+Grammar  NaN  NaN\n",
       "fasttext+Personality+Interactions          NaN  NaN\n",
       "fasttext+Personality+Grammar               NaN  NaN"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns=[\"Mean\", \"SD\"]\n",
    "tested=[\"Bag-of-Words_500\",\n",
    "        \"Bag-of-Words_1000\",\n",
    "        \"fastText\",\n",
    "        \"Personality\",\n",
    "        \"Grammar\",\n",
    "        \"Sentiment\",\n",
    "        \"Interaction\",\n",
    "        \"Personality+Grammar+Interaction\",\n",
    "        \"BOW+Personality\",\n",
    "        \"BOW+Personality+Interactions\",\n",
    "        \"BOW+Personality+Interactions+Grammar\",\n",
    "        \"BOW+Personality+Grammar\",\n",
    "        \"fasttext+Personality\",\n",
    "        \"fasttext+Personality+Interactions+Grammar\",\n",
    "        \"fasttext+Personality+Interactions\",\n",
    "        \"fasttext+Personality+Grammar\"\n",
    "       ]\n",
    "result_df=pd.DataFrame(columns=columns, index=tested)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "# FULL Version: Use this for Logit \n",
    "def create_results(results,n, model):\n",
    "    '''This functions shows the mean accuracy over n samples.'''\n",
    "    df=x_m\n",
    "    \n",
    "    # 1 BoW\n",
    "    acc=[]\n",
    "    for seed in range(n):\n",
    "        acc_n=prediction(df, bow=True,top_words=500,seed=seed,model=model)\n",
    "        acc.append(acc_n)\n",
    "    mean = sum(acc)/len(acc)\n",
    "    sd = statistics.stdev(acc)\n",
    "    results.loc[\"Bag-of-Words_500\",\"Mean\"] = mean\n",
    "    results.loc[\"Bag-of-Words_500\",\"SD\"] = sd\n",
    "\n",
    "    # 2 Tfidf-1000\n",
    "    acc=[]\n",
    "    for seed in range(n):\n",
    "        acc_n=prediction(df, bow=True,top_words=1000,seed=seed,model=model)\n",
    "        acc.append(acc_n)\n",
    "    mean = sum(acc)/len(acc)\n",
    "    sd = statistics.stdev(acc)\n",
    "    results.loc[\"Bag-of-Words_1000\",\"Mean\"] = mean\n",
    "    results.loc[\"Bag-of-Words_1000\",\"SD\"] = sd\n",
    "\n",
    "    # 3 fasttext-500\n",
    "    acc=[]\n",
    "    for seed in range(n):\n",
    "        acc_n=prediction(df, fasttext=True,size=500,seed=seed,model=model)\n",
    "        acc.append(acc_n)\n",
    "    mean = sum(acc)/len(acc)\n",
    "    sd = statistics.stdev(acc)\n",
    "    results.loc[\"fastText\",\"Mean\"] = mean\n",
    "    results.loc[\"fastText\",\"SD\"] = sd\n",
    "\n",
    "    # 4 personality\n",
    "    acc=[]\n",
    "    for seed in range(n):\n",
    "        acc_n=prediction(df, personality=True,seed=seed,model=model)\n",
    "        acc.append(acc_n)\n",
    "    mean = sum(acc)/len(acc)\n",
    "    sd = statistics.stdev(acc)\n",
    "    results.loc[\"Personality\",\"Mean\"] = mean\n",
    "    results.loc[\"Personality\",\"SD\"] = sd\n",
    "\n",
    " \n",
    "    # 5 Personality genre interaction\n",
    "    acc=[]\n",
    "    for seed in range(n):\n",
    "        acc_n=prediction(df, interaction=True,seed=seed,model=model)\n",
    "        acc.append(acc_n)\n",
    "    mean = sum(acc)/len(acc)\n",
    "    sd = statistics.stdev(acc)\n",
    "    results.loc[\"Interaction\",\"Mean\"] = mean\n",
    "    results.loc[\"Interaction\",\"SD\"] = sd\n",
    "\n",
    "    # 6 Grammar\n",
    "    acc=[]\n",
    "    for seed in range(n):\n",
    "        acc_n=prediction(df, grammar=True,seed=seed,model=model)\n",
    "        acc.append(acc_n)\n",
    "    mean = sum(acc)/len(acc)\n",
    "    sd = statistics.stdev(acc)\n",
    "    results.loc[\"Grammar\",\"Mean\"] = mean\n",
    "    results.loc[\"Grammar\",\"SD\"] = sd\n",
    "    \n",
    "    acc=[]\n",
    "    for seed in range(n):\n",
    "        acc_n=prediction(df, personality=True,interaction=True,grammar=True,seed=seed,model=model)\n",
    "        acc.append(acc_n)\n",
    "    mean = sum(acc)/len(acc)\n",
    "    sd = statistics.stdev(acc)\n",
    "    results.loc[\"Personality+Grammar+Interaction\",\"Mean\"] = mean\n",
    "    results.loc[\"Personality+Grammar+Interaction\",\"SD\"] = sd    \n",
    "    \n",
    "    \n",
    "    # 7 BOW_plus_personality\n",
    "       \n",
    "    \n",
    "    acc=[] \n",
    "    for seed in range(n):\n",
    "        acc_n = prediction(df, bow_plus_personality = True ,seed=seed,model=model)\n",
    "        acc.append(acc_n)\n",
    "    mean = sum(acc)/len(acc)\n",
    "    sd = statistics.stdev(acc)\n",
    "    results.loc[\"BOW+Personality\",\"Mean\"] = mean\n",
    "    results.loc[\"BOW+Personality\",\"SD\"] = sd\n",
    "    \n",
    "     # 8 BOW_plus_personality_plus_interactions\n",
    "        \n",
    "    acc=[] \n",
    "    for seed in range(n):\n",
    "        acc_n = prediction(df, bow_plus_personality_plus_interactions = True ,seed=seed,model=model)\n",
    "        acc.append(acc_n)\n",
    "    mean = sum(acc)/len(acc)\n",
    "    sd = statistics.stdev(acc)\n",
    "    results.loc[ \"BOW+Personality+Interactions\",\"Mean\"] = mean\n",
    "    results.loc[ \"BOW+Personality+Interactions\",\"SD\"] = sd\n",
    "    \n",
    "    # 9 BOW_plus_personality_plus_interactions_plus_grammar\n",
    "        \n",
    "    acc=[] \n",
    "    for seed in range(n):\n",
    "        acc_n = prediction(df, bow_plus_personality_plus_grammar = True ,seed=seed,model=model)\n",
    "        acc.append(acc_n)\n",
    "    mean = sum(acc)/len(acc)\n",
    "    sd = statistics.stdev(acc)\n",
    "    results.loc[ \"BOW+Personality+Grammar\",\"Mean\"] = mean\n",
    "    results.loc[ \"BOW+Personality+Grammar\",\"SD\"] = sd\n",
    "    \n",
    "    acc=[] \n",
    "    for seed in range(n):\n",
    "        acc_n = prediction(df, bow_plus_personality_plus_interactions_plus_grammar = True ,seed=seed,model=model)\n",
    "        acc.append(acc_n)\n",
    "    mean = sum(acc)/len(acc)\n",
    "    sd = statistics.stdev(acc)\n",
    "    results.loc[ \"BOW+Personality+Interactions+Grammar\",\"Mean\"] = mean\n",
    "    results.loc[ \"BOW+Personality+Interactions+Grammar\",\"SD\"] = sd\n",
    "    \n",
    "     10 VADER sentiment\n",
    "    acc=[] \n",
    "    for seed in range(n):\n",
    "        acc_n = prediction(df, sentiment = True ,seed=seed,model=model)\n",
    "        acc.append(acc_n)\n",
    "    mean = sum(acc)/len(acc)\n",
    "    sd = statistics.stdev(acc)\n",
    "    results.loc[ \"Sentiment\",\"Mean\"] = mean\n",
    "    results.loc[ \"Sentiment\",\"SD\"] = sd\n",
    "    \n",
    "    \n",
    "    # 11 fasttext_plus_personality\n",
    "       \n",
    "    acc=[] \n",
    "    for seed in range(n):\n",
    "        acc_n = prediction(df, fasttext = True , personality=True, size=500,\n",
    "               bow=False, top_words=500,\n",
    "               grammar=False, \n",
    "               sentiment=False,\n",
    "               interaction = False,\n",
    "               bow_plus_personality = False,\n",
    "               bow_plus_personality_plus_interactions = False,\n",
    "               bow_plus_personality_plus_grammar=False,\n",
    "               bow_plus_personality_plus_interactions_plus_grammar = False,seed=seed,model=model)\n",
    "        acc.append(acc_n)\n",
    "    mean = sum(acc)/len(acc)\n",
    "    sd = statistics.stdev(acc)\n",
    "    results.loc[\"fasttext+Personality\",\"Mean\"] = mean\n",
    "    results.loc[\"fasttext+Personality\",\"SD\"] = sd\n",
    "    \n",
    "     # 12 BOW_plus_personality_plus_interactions\n",
    "        \n",
    "    acc=[] \n",
    "    for seed in range(n):\n",
    "        acc_n = prediction(df, fasttext = True ,personality=True, interaction=True,size=500,\n",
    "               bow=False, top_words=500,\n",
    "               grammar=False, \n",
    "               sentiment=False,\n",
    "               bow_plus_personality = False,\n",
    "               bow_plus_personality_plus_interactions = False,\n",
    "               bow_plus_personality_plus_grammar=False,\n",
    "               bow_plus_personality_plus_interactions_plus_grammar = False,seed=seed,model=model)\n",
    "        acc.append(acc_n)\n",
    "    mean = sum(acc)/len(acc)\n",
    "    sd = statistics.stdev(acc)\n",
    "    results.loc[ \"fasttext+Personality+Interactions\",\"Mean\"] = mean\n",
    "    results.loc[ \"fasttext+Personality+Interactions\",\"SD\"] = sd\n",
    "    \n",
    "    # 9 BOW_plus_personality_plus_interactions_plus_grammar\n",
    "        \n",
    "    acc=[] \n",
    "    for seed in range(n):\n",
    "        acc_n = prediction(df, fasttext = True ,personality=True,grammar=True, size=500,\n",
    "               bow=False, top_words=500,\n",
    "               sentiment=False,\n",
    "               interaction = False,\n",
    "               bow_plus_personality = False,\n",
    "               bow_plus_personality_plus_interactions = False,\n",
    "               bow_plus_personality_plus_grammar=False,\n",
    "               bow_plus_personality_plus_interactions_plus_grammar = False,seed=seed,model=model)\n",
    "        acc.append(acc_n)\n",
    "    mean = sum(acc)/len(acc)\n",
    "    sd = statistics.stdev(acc)\n",
    "    results.loc[ \"fasttext+Personality+Grammar\",\"Mean\"] = mean\n",
    "    results.loc[ \"fasttext+Personality+Grammar\",\"SD\"] = sd\n",
    "    \n",
    "    acc=[] \n",
    "    for seed in range(n):\n",
    "        acc_n = prediction(df, fasttext = True ,personality=True,interaction=True, size=500,\n",
    "               bow=False, top_words=500,\n",
    "               grammar=True, \n",
    "               sentiment=False,\n",
    "               bow_plus_personality = False,\n",
    "               bow_plus_personality_plus_interactions = False,\n",
    "               bow_plus_personality_plus_grammar=False,\n",
    "               bow_plus_personality_plus_interactions_plus_grammar = False,seed=seed,model=model)\n",
    "        acc.append(acc_n)\n",
    "    mean = sum(acc)/len(acc)\n",
    "    sd = statistics.stdev(acc)\n",
    "    results.loc[ \"fasttext+Personality+Interactions+Grammar\",\"Mean\"] = mean\n",
    "    results.loc[ \"fasttext+Personality+Interactions+Grammar\",\"SD\"] = sd\n",
    "        \n",
    "    \n",
    "    return(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "#Smaller Version: For SVM and NB\n",
    "def create_results2(results,n, model):\n",
    "    '''This functions shows the mean accuracy over n samples.'''\n",
    "    df=x_m\n",
    "    \n",
    "    # 1 BoW\n",
    "    acc=[]\n",
    "    for seed in range(n):\n",
    "        acc_n=prediction(df, bow=True,top_words=500,seed=seed,model=model)\n",
    "        acc.append(acc_n)\n",
    "    mean = sum(acc)/len(acc)\n",
    "    sd = statistics.stdev(acc)\n",
    "    results.loc[\"Bag-of-Words_500\",\"Mean\"] = mean\n",
    "    results.loc[\"Bag-of-Words_500\",\"SD\"] = sd\n",
    "\n",
    "    # 2 Tfidf-1000\n",
    "    acc=[]\n",
    "    for seed in range(n):\n",
    "        acc_n=prediction(df, bow=True,top_words=1000,seed=seed,model=model)\n",
    "        acc.append(acc_n)\n",
    "    mean = sum(acc)/len(acc)\n",
    "    sd = statistics.stdev(acc)\n",
    "    results.loc[\"Bag-of-Words_1000\",\"Mean\"] = mean\n",
    "    results.loc[\"Bag-of-Words_1000\",\"SD\"] = sd\n",
    "\n",
    "    # 3 fasttext-500\n",
    "    acc=[]\n",
    "    for seed in range(n):\n",
    "        acc_n=prediction(df, fasttext=True,size=500,seed=seed,model=model)\n",
    "        acc.append(acc_n)\n",
    "    mean = sum(acc)/len(acc)\n",
    "    sd = statistics.stdev(acc)\n",
    "    results.loc[\"fastText\",\"Mean\"] = mean\n",
    "    results.loc[\"fastText\",\"SD\"] = sd\n",
    "\n",
    "    # 4 personality\n",
    "    acc=[]\n",
    "    for seed in range(n):\n",
    "        acc_n=prediction(df, personality=True,seed=seed,model=model)\n",
    "        acc.append(acc_n)\n",
    "    mean = sum(acc)/len(acc)\n",
    "    sd = statistics.stdev(acc)\n",
    "    results.loc[\"Personality\",\"Mean\"] = mean\n",
    "    results.loc[\"Personality\",\"SD\"] = sd\n",
    "\n",
    " \n",
    "    # 5 Personality genre interaction\n",
    "    acc=[]\n",
    "    for seed in range(n):\n",
    "        acc_n=prediction(df, interaction=True,seed=seed,model=model)\n",
    "        acc.append(acc_n)\n",
    "    mean = sum(acc)/len(acc)\n",
    "    sd = statistics.stdev(acc)\n",
    "    results.loc[\"Interaction\",\"Mean\"] = mean\n",
    "    results.loc[\"Interaction\",\"SD\"] = sd\n",
    "\n",
    "    # 6 Grammar\n",
    "    acc=[]\n",
    "    for seed in range(n):\n",
    "        acc_n=prediction(df, grammar=True,seed=seed,model=model)\n",
    "        acc.append(acc_n)\n",
    "    mean = sum(acc)/len(acc)\n",
    "    sd = statistics.stdev(acc)\n",
    "    results.loc[\"Grammar\",\"Mean\"] = mean\n",
    "    results.loc[\"Grammar\",\"SD\"] = sd\n",
    "    \n",
    "    acc=[]\n",
    "    for seed in range(n):\n",
    "        acc_n=prediction(df, personality=True,interaction=True,grammar=True,seed=seed,model=model)\n",
    "        acc.append(acc_n)\n",
    "    mean = sum(acc)/len(acc)\n",
    "    sd = statistics.stdev(acc)\n",
    "    results.loc[\"Personality+Grammar+Interaction\",\"Mean\"] = mean\n",
    "    results.loc[\"Personality+Grammar+Interaction\",\"SD\"] = sd    \n",
    "    \n",
    "    \n",
    "    return(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resu=create_results(result_df,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resu25 = create_results(result_df,10)\n",
    "resu25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resu25.to_excel(\"results_07_08.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resuNN=create_results2(result_df,2,model=\"nn\")\n",
    "resuNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resuNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18808, 500)\n",
      "(18808, 500)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>SD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Bag-of-Words_500</th>\n",
       "      <td>0.773525</td>\n",
       "      <td>0.00526289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bag-of-Words_1000</th>\n",
       "      <td>0.789607</td>\n",
       "      <td>0.00394717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fastText</th>\n",
       "      <td>0.645534</td>\n",
       "      <td>0.000563881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Personality</th>\n",
       "      <td>0.587719</td>\n",
       "      <td>0.0105258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Grammar</th>\n",
       "      <td>0.615497</td>\n",
       "      <td>0.00169164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentiment</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Interaction</th>\n",
       "      <td>0.56446</td>\n",
       "      <td>0.0125933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Personality+Grammar+Interaction</th>\n",
       "      <td>0.576954</td>\n",
       "      <td>0.00921006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BOW+Personality</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BOW+Personality+Interactions</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BOW+Personality+Interactions+Grammar</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BOW+Personality+Grammar</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fasttext+Personality</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fasttext+Personality+Interactions+Grammar</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fasttext+Personality+Interactions</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fasttext+Personality+Grammar</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Mean           SD\n",
       "Bag-of-Words_500                           0.773525   0.00526289\n",
       "Bag-of-Words_1000                          0.789607   0.00394717\n",
       "fastText                                   0.645534  0.000563881\n",
       "Personality                                0.587719    0.0105258\n",
       "Grammar                                    0.615497   0.00169164\n",
       "Sentiment                                       NaN          NaN\n",
       "Interaction                                 0.56446    0.0125933\n",
       "Personality+Grammar+Interaction            0.576954   0.00921006\n",
       "BOW+Personality                                 NaN          NaN\n",
       "BOW+Personality+Interactions                    NaN          NaN\n",
       "BOW+Personality+Interactions+Grammar            NaN          NaN\n",
       "BOW+Personality+Grammar                         NaN          NaN\n",
       "fasttext+Personality                            NaN          NaN\n",
       "fasttext+Personality+Interactions+Grammar       NaN          NaN\n",
       "fasttext+Personality+Interactions               NaN          NaN\n",
       "fasttext+Personality+Grammar                    NaN          NaN"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resuNB=create_results2(result_df,2,model=\"nb\")\n",
    "resuNB\n",
    "#10:12 -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>SD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Bag-of-Words_500</th>\n",
       "      <td>0.773525</td>\n",
       "      <td>0.00526289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bag-of-Words_1000</th>\n",
       "      <td>0.789607</td>\n",
       "      <td>0.00394717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fastText</th>\n",
       "      <td>0.645534</td>\n",
       "      <td>0.000563881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Personality</th>\n",
       "      <td>0.587719</td>\n",
       "      <td>0.0105258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Grammar</th>\n",
       "      <td>0.615497</td>\n",
       "      <td>0.00169164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentiment</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Interaction</th>\n",
       "      <td>0.56446</td>\n",
       "      <td>0.0125933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Personality+Grammar+Interaction</th>\n",
       "      <td>0.576954</td>\n",
       "      <td>0.00921006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BOW+Personality</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BOW+Personality+Interactions</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BOW+Personality+Interactions+Grammar</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BOW+Personality+Grammar</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fasttext+Personality</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fasttext+Personality+Interactions+Grammar</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fasttext+Personality+Interactions</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fasttext+Personality+Grammar</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Mean           SD\n",
       "Bag-of-Words_500                           0.773525   0.00526289\n",
       "Bag-of-Words_1000                          0.789607   0.00394717\n",
       "fastText                                   0.645534  0.000563881\n",
       "Personality                                0.587719    0.0105258\n",
       "Grammar                                    0.615497   0.00169164\n",
       "Sentiment                                       NaN          NaN\n",
       "Interaction                                 0.56446    0.0125933\n",
       "Personality+Grammar+Interaction            0.576954   0.00921006\n",
       "BOW+Personality                                 NaN          NaN\n",
       "BOW+Personality+Interactions                    NaN          NaN\n",
       "BOW+Personality+Interactions+Grammar            NaN          NaN\n",
       "BOW+Personality+Grammar                         NaN          NaN\n",
       "fasttext+Personality                            NaN          NaN\n",
       "fasttext+Personality+Interactions+Grammar       NaN          NaN\n",
       "fasttext+Personality+Interactions               NaN          NaN\n",
       "fasttext+Personality+Grammar                    NaN          NaN"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resuNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "resuNB.to_excel(\"results_10_08_NB.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>SD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Bag-of-Words_500</th>\n",
       "      <td>0.773525</td>\n",
       "      <td>0.00526289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bag-of-Words_1000</th>\n",
       "      <td>0.789607</td>\n",
       "      <td>0.00394717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fastText</th>\n",
       "      <td>0.645534</td>\n",
       "      <td>0.000563881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Personality</th>\n",
       "      <td>0.587719</td>\n",
       "      <td>0.0105258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Grammar</th>\n",
       "      <td>0.615497</td>\n",
       "      <td>0.00169164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentiment</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Interaction</th>\n",
       "      <td>0.56446</td>\n",
       "      <td>0.0125933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Personality+Grammar+Interaction</th>\n",
       "      <td>0.685646</td>\n",
       "      <td>0.00544357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BOW+Personality</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BOW+Personality+Interactions</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BOW+Personality+Interactions+Grammar</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BOW+Personality+Grammar</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fasttext+Personality</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fasttext+Personality+Interactions+Grammar</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fasttext+Personality+Interactions</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fasttext+Personality+Grammar</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Mean           SD\n",
       "Bag-of-Words_500                           0.773525   0.00526289\n",
       "Bag-of-Words_1000                          0.789607   0.00394717\n",
       "fastText                                   0.645534  0.000563881\n",
       "Personality                                0.587719    0.0105258\n",
       "Grammar                                    0.615497   0.00169164\n",
       "Sentiment                                       NaN          NaN\n",
       "Interaction                                 0.56446    0.0125933\n",
       "Personality+Grammar+Interaction            0.685646   0.00544357\n",
       "BOW+Personality                                 NaN          NaN\n",
       "BOW+Personality+Interactions                    NaN          NaN\n",
       "BOW+Personality+Interactions+Grammar            NaN          NaN\n",
       "BOW+Personality+Grammar                         NaN          NaN\n",
       "fasttext+Personality                            NaN          NaN\n",
       "fasttext+Personality+Interactions+Grammar       NaN          NaN\n",
       "fasttext+Personality+Interactions               NaN          NaN\n",
       "fasttext+Personality+Grammar                    NaN          NaN"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#10;20\n",
    "from sklearn.svm import SVC\n",
    "resuSVM=create_results2(result_df,10,model=\"svm\")\n",
    "resuSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "resuSVM.to_excel(\"results_10_08_SVM.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
